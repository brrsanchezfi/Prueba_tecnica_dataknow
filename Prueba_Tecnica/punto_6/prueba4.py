# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pQtMcRwXoe7OqGWQRIZjmpyQnA1DA4tD
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('/train.csv')
data.head(5)

data.describe()

number_fraude = len(data[data.FRAUDE == 1])
number_normal = len(data[data.FRAUDE == 0])

print ("Fraude:", number_fraude)
print ("Normal:", number_normal)

"""**DATA CLEANING**


"""

# Identificar valores nulos
print(data.isnull().sum())

# Hipotesis: si numero de paises es igual a '1' las medidas internacionales aparecen como 'NAN', por tanto es factible remplezar esos

# Filtrar las filas donde NROPAISES es igual a 1
subset_data = data[data['NROPAISES'] == 1]

# Verificar si las columnas son nulas en el subconjunto
print("Número de filas donde NROPAISES es 1:", len(subset_data))

# Comprobar si las columnas son nulas
print("Número de filas donde Dist_Sum_INTER es nulo:", subset_data['Dist_Sum_INTER'].isnull().sum())
print("Número de filas donde Dist_Mean_INTER es nulo:", subset_data['Dist_Mean_INTER'].isnull().sum())
print("Número de filas donde Dist_Max_INTER es nulo:", subset_data['Dist_Max_INTER'].isnull().sum())

# Seria valido reemplzar los NAN por 0
data['Dist_Sum_INTER'].fillna(0, inplace=True)
data['Dist_Mean_INTER'].fillna(0, inplace=True)
data['Dist_Max_INTER'].fillna(0, inplace=True)

# Filtrar las filas con valores nulos en las columnas mencionadas
filas_nulas = data.loc[data['FECHA_VIN'].isnull() | data['OFICINA_VIN'].isnull() | data['SEGMENTO'].isnull() | data['EDAD'].isnull() | data['INGRESOS'].isnull() | data['EGRESOS'].isnull()]

# Mostrar las filas que cumplen con la condición
filas_nulas.isnull().sum()

#La hipotesis parece correcta, y tales datos podrian corresponder a datos de transacciones fallidas

transacciones_normales_nulos = filas_nulas[filas_nulas['FRAUDE'] == 0].shape[0]

print('Transacciones normales entre las filas nulas: ', transacciones_normales_nulos)

# Eliminar filas nulas de 'filas_nulas' en el DataFrame original 'data'
data_filter = data.drop(filas_nulas.index)

data=data_filter
# suficiente para descartar todas estas columnas nulas incluyendo la que aparece como fraude, por tanto descartamos estas columnas

# Hipotesis: si el NROCIUDADES (numero de ciudades) es 1, Dist_Mean_NAL es NAN, si la hiposis es correcta se puede cambiar los valores NAN como '0'


# Filtrar las filas donde NROCIUDADES es igual a 1
subset_data = data[data['NROCIUDADES'] == 1]

# Verificar si las columnas son nulas en el subconjunto
print("Número de filas donde NROCIUDADES es 1:", len(subset_data))

# Comprobar si las columnas son nulas
print("Número de filas donde Dist_Mean_NAL es nulo:", subset_data['Dist_Mean_NAL'].isnull().sum())

#La hipotesis es correcta, cambiamos '0' por 'NAN'

data['Dist_Mean_NAL'].fillna(0, inplace=True)

"""Por ultimo faltaria hacer un tratamiento al los datos que no reportan sexo, no obstante al ser datos categoricos ya no binario, y ser relativamente pocos podriamos implementar cualquier metodo, sin embargo para efectors practicos y teniendo en cuentas que son relativamente pocos datos simplemente los borrare."""

filas_nulas_sexo = data[data['SEXO'].isnull()]

# Contar la cantidad de fraudes en este subconjunto
fraudes_en_filas_nulas = filas_nulas_sexo['FRAUDE'].sum()
print('Número de fraudes en filas con sexo nulo:', fraudes_en_filas_nulas)

data = data.dropna(subset=['SEXO'])

print(data.isnull().sum())





# Suponiendo que 'data_sin_nulas' es tu DataFrame sin filas nulas
data.to_csv('/data_preliminar.csv', index=False)

data.info()

"""**MODELO DE ML**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


def split_data(data, target_column='FRAUDE', test_size=0.1, random_state=42):
    """Dividir el conjunto de datos en entrenamiento y prueba."""
    X = data.drop(target_column, axis=1)
    y = data[target_column]
    return train_test_split(X, y, test_size=test_size, random_state=random_state)

def build_preprocessor(numeric_features, categorical_features):
    """Construir el preprocesador utilizando ColumnTransformer."""
    numeric_transformer = Pipeline(steps=[
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    return ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ])

def build_model(preprocessor, classifier=LogisticRegression()):
    """Construir el modelo de clasificación."""
    return Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', classifier)
    ])

def train_model(model, X_train, y_train):
    """Entrenar el modelo."""
    model.fit(X_train, y_train)

def evaluate_model(model, X_test, y_test):
    """Evaluar el modelo y devolver la precisión."""
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy


# Dividir datos en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = split_data(data)

# Definir columnas numéricas y categóricas
numeric_features = ['VALOR', 'HORA_AUX', 'Dist_max_NAL', 'EDAD', 'INGRESOS', 'EGRESOS', 'NROPAISES', 'Dist_Sum_INTER', 'Dist_Mean_INTER', 'Dist_Max_INTER', 'NROCIUDADES', 'Dist_Mean_NAL', 'Dist_HOY', 'Dist_sum_NAL']
categorical_features = ['Canal1', 'COD_PAIS', 'CANAL', 'DIASEM', 'DIAMES', 'FECHA_VIN', 'OFICINA_VIN', 'SEXO', 'SEGMENTO']

# Construir preprocesador
preprocessor = build_preprocessor(numeric_features, categorical_features)

# Construir y entrenar el modelo
model = build_model(preprocessor)
train_model(model, X_train, y_train)

# Evaluar el modelo
accuracy = evaluate_model(model, X_test, y_test)
print(f'Precisión del modelo: {accuracy}')

from sklearn.metrics import confusion_matrix

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_test, y_pred)


labels = ['Clase 0', 'Clase 1']
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.title('Matriz de Confusión')
plt.xlabel('Predicciones')
plt.ylabel('Clase Real')
plt.show()
# Verdaderos Negativos : 212 casos fueron correctamente clasificados como la clase 0.
# Falsos Positivos : 10 casos fueron incorrectamente clasificados como la clase 1.
# Falsos Negativos : 11 casos fueron incorrectamente clasificados como la clase 0.
# Verdaderos Positivos : 58 casos fueron correctamente clasificados como la clase 1.

"""**TEST**"""

test_data = pd.read_csv('/test.csv')

test_data.rename(columns={'Dist_max_COL': 'Dist_max_NAL'}, inplace=True)

columns_to_select = ['id', 'VALOR', 'HORA_AUX', 'Dist_max_NAL', 'Canal1', 'FECHA', 'COD_PAIS', 'CANAL', 'DIASEM', 'DIAMES', 'FECHA_VIN', 'OFICINA_VIN', 'SEXO', 'SEGMENTO', 'EDAD', 'INGRESOS', 'EGRESOS', 'NROPAISES', 'Dist_Sum_INTER', 'Dist_Mean_INTER', 'Dist_Max_INTER', 'NROCIUDADES', 'Dist_Mean_NAL', 'Dist_HOY', 'Dist_sum_NAL']
new_test_data = test_data[columns_to_select]

#EXISTEN INCOHERENCIAS ENTRE LOS DATOS DE ENTRENAMIENTO Y TEST

condition1 = new_test_data['NROCIUDADES'] == '1'
new_test_data.loc[condition1, ['Dist_Mean_NAL', 'Dist_sum_NAL', 'Dist_max_NAL']] = 0
condition2 = new_test_data['NROPAISES'] == '1'
new_test_data.loc[condition1, ['Dist_Sum_INTER', 'Dist_Mean_INTER', 'Dist_Max_INTER']] = 0

new_test_data.fillna(0, inplace=True)
new_test_data

y_test_processed = model.predict(new_test_data)
y_test_processed

data_test = pd.read_csv('/test.csv')
data_test['FRAUDE'] = y_test_processed
data_test.to_csv('/salida.csv', index=False)